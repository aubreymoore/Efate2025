{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOE\n",
    "# from tqdm import tqdm\n",
    "from icecream import ic\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "# import fiftyone as fo\n",
    "# import fiftyone.utils.yolo as fouy\n",
    "# import supervision as sv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_IMAGE_DIR='../original_images'\n",
    "\n",
    "# dt = datetime.now()\n",
    "# timestamp = f'{dt.year}{dt.month:02d}{dt.day:02d}-{dt.hour:02d}{dt.minute:02d}'\n",
    "# FO_DATASET_NAME = f'after-postproc-{timestamp}'\n",
    "\n",
    "YOLO_ANNOTATION_DIR='../yolo_annotations'\n",
    "os.makedirs(YOLO_ANNOTATION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level to WARNING to suppress INFO logs from ultralytics\n",
    "# logging.getLogger('ultralytics').setLevel(logging.INFO)\n",
    "# logger = logging.getLogger()\n",
    "# logger.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2bcad",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_list_to_yolo_annotation_file(contour_list, width, height, yolo_annotation_path):\n",
    "    \"\"\" \n",
    "    Function called by test_handle_masks.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    Inputs\n",
    "        contour_list: list of contours for detected objects returned by handle_masks()\n",
    "        width: width of original image\n",
    "        height: height of original image\n",
    "        \n",
    "    Output:\n",
    "        yolo_annotation_path: filepath for a YOLO annotation text file to be written by this function\n",
    "    \"\"\"\n",
    "    polygon_str = ''    \n",
    "    for contour in contour_list:\n",
    "        polygon_str += '0 ' # class 0\n",
    "        for point in contour:\n",
    "            x = point[0][0]\n",
    "            y = point[0][1]\n",
    "            normalized_x = x / width\n",
    "            normalized_y = y / height\n",
    "            polygon_str += f' {normalized_x:.4f} {normalized_y:.4f}'\n",
    "        polygon_str += '\\n'        \n",
    "    with open(yolo_annotation_path, 'w') as f:\n",
    "        f.write(polygon_str[:-1]) # write polygon_str after removing last '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e687ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(original_image_path, yolo_annotation_path, annotated_image_path):\n",
    "    \"\"\" \n",
    "    Function used by test_handle_masks.\n",
    "    Gets an image from original_image_path and uses data from yolo_annotation_path to\n",
    "    overlay polygons on it. Resulting image is written to annotated_image_path.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(original_image_path)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    with open(yolo_annotation_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stringlist = line.strip().split()[1:] # class at start of line is removed\n",
    "        numberlist = list(map(float, stringlist))\n",
    "        points_xyn = np.array(numberlist).reshape(-1, 2) # normalized points\n",
    "        points_pix = (points_xyn * np.array([width, height])).astype(int) \n",
    "        img = cv2.polylines(img, pts=[points_pix], color=(0, 255, 0), isClosed=True, thickness=1)\n",
    "    cv2.imwrite(annotated_image_path, img)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_handle_masks(original_image_path, yolo_labels_path):\n",
    "    \"\"\" \n",
    "    Test code for the handle_masks function.\n",
    "    Steps:\n",
    "    1. Detects coconut palms in the image specified by original_image_path using YOLOE.\n",
    "    2. handle_masks() uses the prediction results as input to remove artifacts and \n",
    "    detections which meet the sides or top of the image.\n",
    "    The new masks data are returned as countour_list.\n",
    "    3. contour_list_to_annotation_file() creates a YOLO format annotation text file \n",
    "    from the contour_list data.\n",
    "    4. annotate_image() creates a new image with the new contours overlaid on the \n",
    "    original image and saves the image to a file. \n",
    "    \"\"\"\n",
    "        \n",
    "    # original_image_path = '/home/aubrey/Desktop/inat-coco-jb/images/test_images/66897148.jpg'\n",
    "    # yolo_labels_path = 'yolo_labels/668997148.txt'\n",
    "    # original_image_path = '/home/aubrey/Desktop/inat-coco-jb/images/test_images/117236387.jpg'\n",
    "    # yolo_annotation_path = 'mytest.txt'\n",
    "    # annotated_image_path = f'mytest_{Path(original_image_path).stem}_annotated.jpg'\n",
    "    \n",
    "    # step 1\n",
    "    model = YOLOE(\"yoloe-11l-seg.pt\") \n",
    "    names = [\"coconut palm tree\"] \n",
    "    model.set_classes(names, model.get_text_pe(names))\n",
    "    results = model.predict(source=original_image_path, conf=0.05, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # step 2\n",
    "    mask_list, contour_list = handle_masks(result)\n",
    "    height, width = mask_list[0].shape\n",
    "    ic(len(mask_list), len(contour_list))\n",
    "    ic(mask_list[0].shape)\n",
    "\n",
    "    # create image showing masks\n",
    "    img = np.zeros_like(mask_list[0], dtype=np.uint8)\n",
    "    for i, mask in enumerate(mask_list):\n",
    "        img += (mask//255) * (255-(i*50))\n",
    "        cv2.imwrite('mytest_masks.png', img)\n",
    "\n",
    "    # create image showing contours\n",
    "    img = np.zeros_like(mask_list[0], dtype=np.uint8)\n",
    "    for i, contour in enumerate(contour_list):\n",
    "        img += cv2.polylines(img, pts=[contour], color=(255-i*50), isClosed=True, thickness=1)\n",
    "        cv2.imwrite('mytest_contours.png', img)\n",
    "    \n",
    "    # step 3    \n",
    "    contour_list_to_yolo_annotation_file(contour_list, width, height, yolo_annotation_path)\n",
    "    \n",
    "    # step 4\n",
    "    annotate_image(original_image_path, yolo_annotation_path, annotated_image_path)\n",
    "    \n",
    "# # Usage example:\n",
    "# test_handle_masks(\n",
    "#     original_image_path='/home/aubrey/Desktop/inat-coco-jb/images/test_images/66897148.jpg',\n",
    "#     yolo_annotation_path='yolo_labels/668997148.txt'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_largest_component_opencv(mask):\n",
    "    \"\"\"\n",
    "    Keeps only the largest connected component in a binary mask using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): A binary input mask (0s and 255s).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A new mask containing only the largest connected component (0s and 255s).\n",
    "    \"\"\"\n",
    "    # Ensure mask is in the correct format (uint8 binary)\n",
    "    if mask.dtype != np.uint8:\n",
    "        mask = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Calculate contours\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        return np.zeros_like(mask, dtype=np.uint8), None\n",
    "\n",
    "    # Get the largest contour by area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Create a new blank mask and draw only the largest contour\n",
    "    new_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "    cv2.drawContours(new_mask, [largest_contour], -1, 255, cv2.FILLED)\n",
    "    # ic('keep', largest_contour)\n",
    "    return new_mask, largest_contour\n",
    "\n",
    "# Example Usage:\n",
    "# Assume 'yolo_mask' is your input binary mask (e.g., a NumPy array of 0s and 255s)\n",
    "# yolo_mask = ... \n",
    "# result_mask = keep_largest_component_opencv(yolo_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_touches_left_right_or_top_of_image(mask):\n",
    "    first_column = mask[:, 0] # first column (left)\n",
    "    last_column = mask[:, -1] # last column (right)\n",
    "    first_row = mask[0] # first row (top)\n",
    "    mask_touches_left = np.sum(first_column) > 0\n",
    "    mask_touches_right = np.sum(last_column) > 0\n",
    "    mask_touches_top = np.sum(first_row) > 0\n",
    "    \n",
    "    # The result of the following conditional is np.True_ or np.False\n",
    "    # These values are not equvalant to python's True and False\n",
    "    # Hopfully, the following fixes this problem.\n",
    "    if (mask_touches_left or mask_touches_right or mask_touches_top):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "# # Usage example\n",
    "# # The source image contains 2 mask, the first is OK, the second touches the right edge of the image.\n",
    "# model = YOLOE(\"yoloe-11l-seg.pt\") \n",
    "# names = [\"coconut palm tree\"] \n",
    "# model.set_classes(names, model.get_text_pe(names))\n",
    "# source = '/home/aubrey/Desktop/inat-coco-jb/images/test_images/66897148.jpg'  \n",
    "# results = model.predict(source=source, conf=0.05)\n",
    "# for result in results:\n",
    "#     for mask in result.masks:\n",
    "#         # convert mask from tensor to binary image\n",
    "#         mask = mask.data[0].cpu().numpy() * 255 \n",
    "#         ic(mask_touches_left_right_or_top_of_image(mask))\n",
    "        \n",
    "# # ic| mask_touches_left_right_or_top_of_image(mask): False\n",
    "# # ic| mask_touches_left_right_or_top_of_image(mask): True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_masks(result): \n",
    "    \"\"\" \n",
    "    Postprocesses detections to remove artifacts and objects touching the \n",
    "    sides or top of the image.\n",
    "    \n",
    "    Returns:\n",
    "        mask_list: a list of numpy arrays; each array is a grayscale image \n",
    "        contour_list: a list of contours with format: \n",
    "            [[[x1,y1],[x2,y2],...,[xn,yn]]] where [x,y] defines a pixel location  \n",
    "    \"\"\" \n",
    "    mask_list = []\n",
    "    contour_list = []\n",
    "    if result.masks is None:\n",
    "        return mask_list, contour_list # return emapty lists\n",
    "           \n",
    "    ic('creating blank binary image with same shape as the first mask')\n",
    "    mask = result.masks.data[0].cpu().numpy()\n",
    "    binary_image = np.zeros_like(mask, dtype=np.uint8)\n",
    "    ic(binary_image)\n",
    "    \n",
    "    for mask_num, mask in enumerate(result.masks):\n",
    "        \n",
    "        # convert mask from tensor to binary image\n",
    "        mask = mask.data[0].cpu().numpy() * 255 \n",
    "\n",
    "        # # save mask to image file\n",
    "        # source_filename = os.path.basename(result.path)\n",
    "        # mask_filename = f'{source_filename.split('.')[0]}-{mask_num}.jpg'\n",
    "        # cv2.imwrite(mask_filename, mask)\n",
    "        \n",
    "        # remove artifacts\n",
    "        mask, contour = keep_largest_component_opencv(mask)\n",
    "        # ic('handle_masks', contour)\n",
    "\n",
    "        # if this mask has pixels in the first column (left) or last column (right) or first row (top)\n",
    "        # it is not added to the binary image   \n",
    "        ic(mask_touches_left_right_or_top_of_image(mask))\n",
    "           \n",
    "        if not mask_touches_left_right_or_top_of_image(mask):\n",
    "            # binary_image += mask\n",
    "            mask_list.append(mask)\n",
    "            contour_list.append(contour)\n",
    "    \n",
    "    # ic('saving binary_image to file')     \n",
    "    # binary_image_filename = f\"{source_filename.split('.')[0]}-mask.jpg\"\n",
    "    # if not cv2.imwrite(binary_image_filename, binary_image):\n",
    "    #     ic(f'ERROR: Failed to write binary image to {binary_image_filename}') \n",
    "    \n",
    "    return mask_list, contour_list  \n",
    "\n",
    "# # Usage example:\n",
    "# original_image_path = '/home/aubrey/Desktop/inat-coco-jb/images/test_images/117236387.jpg'\n",
    "# model = YOLOE(\"yoloe-11l-seg.pt\") \n",
    "# names = [\"coconut palm tree\"] \n",
    "# model.set_classes(names, model.get_text_pe(names))\n",
    "# results = model.predict(source=original_image_path, conf=0.05, verbose=False)\n",
    "# result = results[0]\n",
    "# mask_list, contour_list = handle_masks(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segmentation_results_to_yolo_format(\n",
    "    results, \n",
    "    output_labels_dir=\"predicted_seg_labels\",\n",
    "    reject_objects_touching_image_sides_or_top=True,\n",
    "    use_only_largest_contour_per_object=True):\n",
    "    \"\"\" \n",
    "    Save segmentation results in YOLO segmentation format.\n",
    "    \"\"\"    \n",
    "    os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "    \n",
    "        # Extract masks and class indices\n",
    "        masks = result.masks.data if result.masks is not None else None\n",
    "        classes = result.boxes.cls.cpu().numpy() if result.boxes is not None else None\n",
    "\n",
    "        if masks is None or classes is None:\n",
    "            continue\n",
    "        \n",
    "        ic(result.path)\n",
    "\n",
    "        image_name = os.path.basename(result.path)\n",
    "        label_name = os.path.splitext(image_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(output_labels_dir, label_name)\n",
    "        \n",
    "        yolo_annotations = []\n",
    "\n",
    "        # Loop through each instance with a detected mask\n",
    "        for j, mask in enumerate(masks):\n",
    "            class_index = int(classes[j])\n",
    "                            \n",
    "            # Format: class_id x1 y1 x2 y2 ... xn yn \n",
    "            mask_xyn = result.masks.xyn[j]              \n",
    "            mask_xyn_list = mask_xyn.tolist()\n",
    "            flat_mask_xyn_list = [coordinate for point in mask_xyn_list for coordinate in point] \n",
    "            \n",
    "            if use_only_largest_contour_per_object:\n",
    "                new_mask = get_largest_mask(\n",
    "                    filename=os.path.basename(result.path),\n",
    "                    mask=mask,\n",
    "                    mask_num=j,\n",
    "                    output_image_dir='annotated_images/predict')\n",
    "                \n",
    "\n",
    "            if reject_objects_touching_image_sides_or_top:\n",
    "                # Check if any x or y coordinates are at the image sides or top\n",
    "                xs = flat_mask_xyn_list[0::2]\n",
    "                ys = flat_mask_xyn_list[1::2]\n",
    "                if min(xs) < 0.01 or max(xs) > 0.99 or min(ys) < 0.01:\n",
    "                    # ic(f\"Rejecting object {j} in image {i} touching image sides or top\")\n",
    "                    continue\n",
    "                     \n",
    "            polyline_str = \" \".join([f\"{x:.4f}\" for x in flat_mask_xyn_list])        \n",
    "            annotation = f\"{class_index} {polyline_str}\\n\"\n",
    "            yolo_annotations.append(annotation)\n",
    "\n",
    "        # Write annotations to the text file\n",
    "        with open(label_path, \"w\") as f:\n",
    "            f.writelines(yolo_annotations)\n",
    " \n",
    "# # --- Example Usage ---\n",
    "# # 1. Load a segmentation model (e.g., yolov8n-seg.pt)\n",
    "# model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "# # 2. Run inference to get results (ensure you use a segmentation model)\n",
    "# # You can stream results from a directory\n",
    "# results = model(source=\"your/image/directory\", stream=True)\n",
    "\n",
    "# # 3. Save results in the YOLO segmentation format\n",
    "# save_segmentation_results_to_yolo_format(results)\n",
    "# save_segmentation_results_to_yolo_format(\n",
    "#     results, \n",
    "#     output_labels_dir=\"predicted_seg_labels\",\n",
    "#     reject_objects_touching_image_sides_or_top=True,\n",
    "#     use_only_largest_contour_per_object=False):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_yolo_label_txt(result):\n",
    "\n",
    "        image_name = os.path.basename(result.path)\n",
    "        label_name = os.path.splitext(image_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(output_labels_dir, label_name)\n",
    "        \n",
    "        yolo_annotations = []\n",
    "\n",
    "        # Loop through each instance with a detected mask\n",
    "        for j, mask in enumerate(masks):\n",
    "            class_index = int(classes[j])\n",
    "                            \n",
    "            # Format: class_id x1 y1 x2 y2 ... xn yn \n",
    "            mask_xyn = result.masks.xyn[j]              \n",
    "            mask_xyn_list = mask_xyn.tolist()\n",
    "            flat_mask_xyn_list = [coordinate for point in mask_xyn_list for coordinate in point] \n",
    "            \n",
    "            if use_only_largest_contour_per_object:\n",
    "                new_mask = get_largest_mask(\n",
    "                    filename=os.path.basename(result.path),\n",
    "                    mask=mask,\n",
    "                    mask_num=j,\n",
    "                    output_image_dir='annotated_images/predict')\n",
    "                \n",
    "\n",
    "            if reject_objects_touching_image_sides_or_top:\n",
    "                # Check if any x or y coordinates are at the image sides or top\n",
    "                xs = flat_mask_xyn_list[0::2]\n",
    "                ys = flat_mask_xyn_list[1::2]\n",
    "                if min(xs) < 0.01 or max(xs) > 0.99 or min(ys) < 0.01:\n",
    "                    # ic(f\"Rejecting object {j} in image {i} touching image sides or top\")\n",
    "                    continue\n",
    "                     \n",
    "            polyline_str = \" \".join([f\"{x:.4f}\" for x in flat_mask_xyn_list])        \n",
    "            annotation = f\"{class_index} {polyline_str}\\n\"\n",
    "            yolo_annotations.append(annotation)\n",
    "\n",
    "        # Write annotations to the text file\n",
    "        with open(label_path, \"w\") as f:\n",
    "            f.writelines(yolo_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seg_labels_to_polylines(label_path):\n",
    "    \"\"\" \n",
    "    Convert YOLO segmentation labels to FiftyOne polylines.\n",
    "    label_path : path to the YOLO segmentation label file\n",
    "    Returns a list of FiftyOne Polyline objects.\n",
    "    \"\"\"\n",
    "    polylines = []    \n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = parts[0]\n",
    "        coords = list(map(float, parts[1:]))\n",
    "        points = [[(coords[i], coords[i+1]) for i in range(0, len(coords), 2)]]\n",
    "        poly = fo.Polyline(points=points, closed=True, filled=True, label=class_id)\n",
    "        polylines.append(poly)\n",
    "    \n",
    "    return polylines\n",
    "\n",
    "# # --- Example Usage ---\n",
    "# # 1. Convert a YOLO segmentation label file to FiftyOne polylines\n",
    "# label_path = 'predicted_seg_labels/265378773.txt'\n",
    "# polylines = convert_seg_labels_to_polylines(label_path)\n",
    "# ic(polylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ab872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fo_dataset_with_polylines(fo_dataset_name, images_dir, anns_dir):\n",
    "    \"\"\" \n",
    "    Create a FiftyOne dataset with polylines from predicted segmentation labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Create empty dataset\n",
    "    # dataset = fo.Dataset(fo_dataset_name, persistent=True)\n",
    "    \n",
    "\n",
    "    # samples = []\n",
    "    # for img_path in glob.glob(os.path.join(images_dir, \"*\")):\n",
    "    #     if not os.path.isfile(img_path):\n",
    "    #         continue\n",
    "\n",
    "    #     sample = fo.Sample(filepath=img_path)\n",
    "\n",
    "    #     # Derive annotation path from image filename (adapt as needed)\n",
    "    #     stem = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    #     ann_path = os.path.join(anns_dir, stem + \".txt\")\n",
    "    #     if not os.path.exists(ann_path):\n",
    "    #         samples.append(sample)\n",
    "    #         continue\n",
    "\n",
    "    #     # Attach polylines under field \"polylines\"\n",
    "    #     polylines = convert_seg_labels_to_polylines(ann_path)\n",
    "    #     sample[\"polylines\"] = fo.Polylines(polylines=polylines)\n",
    "    #     samples.append(sample)\n",
    " \n",
    "    dataset = fo.Dataset.from_images_dir(\n",
    "        images_dir=IMAGE_DIR,\n",
    "        name=FO_DATASET_NAME,\n",
    "        persistent=True,\n",
    "    ) \n",
    "    fouy.add_yolo_labels(\n",
    "        dataset,\n",
    "        label_field=\"polygons\",          # new field to hold labels\n",
    "        labels_path=YOLO_ANNOTATION_DIR, # directory of .txt files\n",
    "        classes=['coconut-tree'],\n",
    "        label_type=\"polylines\",          # IMPORTANT: load as polygons\n",
    "    )\n",
    "        \n",
    "    # dataset.add_samples(samples)\n",
    "    # try:\n",
    "    #     session = fo.launch_app(dataset, auto=False)\n",
    "    # except:\n",
    "    #     session = fo.launch_app(dataset, auto=False)\n",
    "        \n",
    "# # Example usage:\n",
    "# create_fo_dataset_with_polylines(\n",
    "#     fo_dataset_name=FO_DATASET_NAME,\n",
    "#     images_dir=IMAGE_DIR,\n",
    "#     anns_dir=YOLO_ANNOTATION_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c49fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_xyn_to_wkt(xyn_coords):\n",
    "    \"\"\"\n",
    "    Converts normalized YOLO polygon coordinates to a WKT POLYGON string.\n",
    "\n",
    "    Args:\n",
    "        xyn_coords (list or numpy array): A list of [x, y] points, normalized (0-1).\n",
    "\n",
    "    Returns:\n",
    "        str: The WKT representation of the polygon.\n",
    "    \"\"\"\n",
    "    # if not xyn_coords:\n",
    "    #     return None\n",
    "\n",
    "    # Shapely Polygon expects a list of (x, y) tuples\n",
    "    polygon_coords = [tuple(point) for point in xyn_coords]\n",
    "\n",
    "    # Create a shapely Polygon object\n",
    "    # Note: WKT format requires the first and last point to be the same to close the loop.\n",
    "    # Shapely handles this automatically when creating the Polygon.\n",
    "    try:\n",
    "        polygon = Polygon(polygon_coords)\n",
    "        # Convert the polygon object to a WKT string\n",
    "        wkt_string = shapely.to_wkt(polygon)\n",
    "        return wkt_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Polygon: {e}\")\n",
    "        return None\n",
    "\n",
    "# # --- Example Usage (assuming you have YOLO prediction results) ---\n",
    "\n",
    "# # Mock function to simulate getting xyn results from YOLO\n",
    "# def get_mock_yolo_results():\n",
    "#     # Example normalized coordinates for a single polygon\n",
    "#     # These values are between 0 and 1\n",
    "#     normalized_polygon_points = [\n",
    "#         [0.1, 0.2], [0.3, 0.1], [0.8, 0.4], [0.6, 0.9], [0.1, 0.8]\n",
    "#     ]\n",
    "#     # In a real scenario, this would come from:\n",
    "#     # results = model.predict(source)\n",
    "#     # first_mask_xyn = results[0].masks.xyn[0]\n",
    "#     return normalized_polygon_points\n",
    "\n",
    "# # Get the coordinates\n",
    "# coords = get_mock_yolo_results()\n",
    "\n",
    "# # Convert to WKT\n",
    "# wkt_output = convert_yolo_xyn_to_wkt(coords)\n",
    "\n",
    "# if wkt_output:\n",
    "#     print(f\"YOLO normalized coordinates: {coords}\")\n",
    "#     print(f\"WKT Polygon: {wkt_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48891c8d",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_handle_masks(original_image_path, yolo_annotation_path):\n",
    "    \"\"\" \n",
    "    Test code for the handle_masks function.\n",
    "    Steps:\n",
    "    1. Detects coconut palms in the image specified by original_image_path using YOLOE.\n",
    "    2. handle_masks() uses the prediction results as input to remove artifacts and \n",
    "    detections which meet the sides or top of the image.\n",
    "    The new masks data are returned as countour_list.\n",
    "    3. contour_list_to_annotation_file() creates a YOLO format annotation text file \n",
    "    from the contour_list data.\n",
    "    4. annotate_image() creates a new image with the new contours overlaid on the \n",
    "    original image and saves the image to a file. \n",
    "    \"\"\"\n",
    "        \n",
    "    # original_image_path = '/home/aubrey/Desktop/inat-coco-jb/images/test_images/66897148.jpg'\n",
    "    # yolo_labels_path = 'yolo_labels/668997148.txt'\n",
    "    original_image_path = '/home/aubrey/Desktop/inat-coco-jb/images/test_images/117236387.jpg'\n",
    "    yolo_annotation_path = 'mytest.txt'\n",
    "    annotated_image_path = f'mytest_{Path(original_image_path).stem}_annotated.jpg'\n",
    "    \n",
    "    # step 1\n",
    "    model = YOLOE(\"yoloe-11l-seg.pt\") \n",
    "    names = [\"coconut palm tree\"] \n",
    "    model.set_classes(names, model.get_text_pe(names))\n",
    "    results = model.predict(source=original_image_path, conf=0.05, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # step 2\n",
    "    mask_list, contour_list = handle_masks(result)\n",
    "    height, width = mask_list[0].shape\n",
    "    ic(len(mask_list), len(contour_list))\n",
    "    ic(mask_list[0].shape)\n",
    "\n",
    "    # create image showing masks\n",
    "    img = np.zeros_like(mask_list[0], dtype=np.uint8)\n",
    "    for i, mask in enumerate(mask_list):\n",
    "        img += (mask//255) * (255-(i*50))\n",
    "        cv2.imwrite('mytest_masks.png', img)\n",
    "\n",
    "    # create image showing contours\n",
    "    img = np.zeros_like(mask_list[0], dtype=np.uint8)\n",
    "    for i, contour in enumerate(contour_list):\n",
    "        img += cv2.polylines(img, pts=[contour], color=(255-i*50), isClosed=True, thickness=1)\n",
    "        cv2.imwrite('mytest_contours.png', img)\n",
    "    \n",
    "    # step 3    \n",
    "    contour_list_to_yolo_annotation_file(contour_list, width, height, yolo_annotation_path)\n",
    "    \n",
    "    # step 4\n",
    "    annotate_image(original_image_path, yolo_annotation_path, annotated_image_path)\n",
    "    \n",
    "# # Usage example:\n",
    "# test_handle_masks(\n",
    "#     original_image_path='/home/aubrey/Desktop/inat-coco-jb/images/test_images/66897148.jpg',\n",
    "#     yolo_annotation_path='yolo_labels/668997148.txt'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08099753",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-annotate coconut palms in test_images and save results in YOLO incident segmentation format\n",
    "\n",
    "# inputs: \n",
    "#   test_images/\n",
    "\n",
    "# outputs: \n",
    "#   yoloe-11l-seg.pt\n",
    "#   mobileclip_blt.ts\n",
    "#   annotated_images/\n",
    "#   predicted_seg_labels/\n",
    "\n",
    "############################################\n",
    "logging.info('    initializing YOLOE model')\n",
    "############################################\n",
    "# When first executed, this script will download the YOLOE-11L-Seg model weights (~90MB) and mobileclip_blt.ts (~300MB)\n",
    "# These files are too large for pushing to GitHub so they should be added to .gitignore\n",
    "ic.disable()\n",
    "model = YOLOE(\"yoloe-11l-seg.pt\") \n",
    "names = [\"coconut palm tree\"] \n",
    "model.set_classes(names, model.get_text_pe(names))\n",
    "\n",
    "###########################################\n",
    "logging.info('    detecting coconut palms')\n",
    "###########################################\n",
    "ic.enable()\n",
    "data_list = []\n",
    "image_paths = sorted(glob.iglob(f'{ORIGINAL_IMAGE_DIR}/*.jpg'))\n",
    "for image_num, image_path in enumerate(image_paths):\n",
    "    if image_num == 100:\n",
    "        break\n",
    "    results = model.predict(\n",
    "        source=image_path, \n",
    "        imgsz=960,\n",
    "        conf=0.01, \n",
    "        save=True, \n",
    "        project='annotated_images', \n",
    "        name='', \n",
    "        exist_ok=True, \n",
    "        verbose=False, \n",
    "        stream=False,\n",
    "        save_crop=True\n",
    "    )\n",
    "    \n",
    "    result = results[0]\n",
    "    ic(image_num, '--------------------------------')\n",
    "    \n",
    "    if result.boxes is not None:  \n",
    "        for  box_id, box in enumerate(result.boxes.xyxy):\n",
    "            ic(box_id, '---')\n",
    "            data_dict = {\n",
    "                'image_id': Path(result.path).stem,\n",
    "                'box_id': box_id,\n",
    "                'conf': result.boxes.conf[box_id].item(),\n",
    "                'polygon': convert_yolo_xyn_to_wkt(result.masks.xyn[box_id])\n",
    "            }\n",
    "            # ic(data_dict)\n",
    "            # ic('before appending', data_list)\n",
    "            # ic('appending', data_dict)\n",
    "            data_list.append(data_dict)\n",
    "            # ic('after_appending', data_list)\n",
    "        \n",
    "ic(data_list)  \n",
    "df = pd.DataFrame(data_list)\n",
    "df\n",
    "\n",
    "    \n",
    "    # if result.masks is None:\n",
    "    #     ic('No masks')\n",
    "    #     continue\n",
    "    # mask_list, contour_list = handle_masks(result)\n",
    "    # ic('main', contour_list)\n",
    "    # ic(len(mask_list), len(contour_list))\n",
    " \n",
    "######################################   \n",
    "# create and save yolo annotation file\n",
    "######################################  \n",
    "    # if len(mask_list) > 0:\n",
    "    #     height, width = mask_list[0].shape \n",
    "    # yolo_annotation_path = f'{YOLO_ANNOTATION_DIR}/{Path(result.path).stem}.txt'    \n",
    "    # contour_list_to_yolo_annotation_file(\n",
    "    #     contour_list = contour_list, \n",
    "    #     width=width, \n",
    "    #     height=height, \n",
    "    #     yolo_annotation_path=yolo_annotation_path\n",
    "    # )\n",
    "   \n",
    "# ic('saving predicted segmentation labels')\n",
    "# save_segmentation_results_to_yolo_format(\n",
    "#     results, \n",
    "#     output_labels_dir=\"predicted_seg_labels\",\n",
    "#     reject_objects_touching_image_sides_or_top=True,\n",
    "#     use_only_largest_contour_per_object=True)\n",
    "\n",
    "# ############################################################\n",
    "# logging.info('    creating fiftyone dataset with polylines')\n",
    "# ############################################################\n",
    "# create_fo_dataset_with_polylines(\n",
    "#     fo_dataset_name=FO_DATASET_NAME,\n",
    "#     images_dir=IMAGE_DIR,\n",
    "#     anns_dir=YOLO_ANNOTATION_DIR)\n",
    "\n",
    "# # ##########################################\n",
    "# # logging.info('    launching FiftyOne app')\n",
    "# # ##########################################\n",
    "# # try:\n",
    "# #     fo.launch_app(FO_DATASET_NAME, auto=False)\n",
    "# # except:\n",
    "# #     fo.launch_app(FO_DATASET_NAME, auto=False)\n",
    "    \n",
    "#############################\n",
    "logging.info('    finished');\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic(result.masks.xyn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff472d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e20d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a12df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    ic(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c277b595",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "create_fo_dataset_with_polylines(\n",
    "    fo_dataset_name=FO_DATASET_NAME,\n",
    "    images_dir=IMAGE_DIR,\n",
    "    anns_dir=YOLO_ANNOTATION_DIR)\n",
    "\n",
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b7d319e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dataset = fo.load_dataset(FO_DATASET_NAME)\n",
    "fo.launch_app(dataset, auto=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Efate2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
